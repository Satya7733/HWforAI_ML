# XOR Feedâ€‘Forward MLP with Backpropagation

A selfâ€‘contained Python implementation of a 2â€‘2â€‘1 multiâ€‘layer perceptron trained via backpropagation to solve the XOR problem, with animated visualization of learning dynamics.

---

## ğŸš€ Features

- **Simple, fromâ€‘scratch implementation**  
  No deepâ€‘learning frameworksâ€”only NumPy and Matplotlib.
- **2â€‘2â€‘1 network architecture**  
  - 2 input neurons  
  - 2 hidden neurons (sigmoid activation)  
  - 1 output neuron (sigmoid activation)
- **Backpropagation training**  
  - Configurable learning rate  
  - MSEâ€‘style weight updates  
  - Snapshot of weights every 100 epochs
- **Visualization**  
  - Static plot of the network topology  
  - Animated decision surface over the 2D input space  
  - Hiddenâ€‘layer â€œdecision linesâ€ drawn as they rotate to carve out XOR
- **Console output**  
  - ASCIIâ€‘diagram of all weights & biases before and after training

---

## ğŸ“‹ Prerequisites

- PythonÂ 3.7+  
- [NumPy](https://numpy.org/)  
- [Matplotlib](https://matplotlib.org/)  
- [Pillow](https://python-pillow.org/) (to save GIFs)

Install dependencies with:

```bash
pip install numpy matplotlib pillow
```

---

## ğŸ—‚ï¸ Repository Structure

```
.
â”œâ”€â”€ xor_mlp_backprop.py     # Main script
â”œâ”€â”€ xor_training.gif        # Generated animation output
â”œâ”€â”€ README.md               # This file

```

---

## âš™ï¸ Usage

1. **Run the script**  
   ```bash
   python xor_mlp_backprop.py
   ```
2. **What happens**  
   - A static window pops up showing the 2â€‘2â€‘1 network layout.  
   - Console prints initial and final weights/biases.  
   - An animated Matplotlib window shows the decision surface evolving.  
   - A `xor_training.gif` is saved in your working directory.  
3. **View results**  
   - Open `xor_training.gif` in any image viewer.  
   - Inspect printed weight values to see how training shaped the network.

---

## ğŸ“ Script Breakdown

1. **`draw_structure()`**  
   - Renders the network graph: input â†’ hidden â†’ output neurons.  
2. **`print_network()`**  
   - Consoleâ€‘prints every connection weight and bias in arrow format.  
3. **Data setup**  
   - Defines the four XOR inputs and targets.  
4. **Model initialization**  
   - Random weights and biases in range [â€“1,1], seeded for reproducibility.  
5. **Training loop**  
   - Forward pass â†’ compute activations.  
   - Backpropagate errors â†’ update weights with learning rate.  
   - Record snapshots every 100 epochs.  
6. **Visualization grid**  
   - Builds a fine mesh over [â€“0.1,1.1]Â² to colorâ€‘map network outputs.  
7. **Animation**  
   - Uses Matplotlibâ€™s `FuncAnimation` to animate snapshots.  
   - Shows how hiddenâ€‘layer decision lines swivel over training.  
8. **Output**  
   - Saves `xor_training.gif` via `PillowWriter`.  
   - Pops up final animation plot with `plt.show()`.

---

## ğŸ§  Network Architecture

```
  x0â”€â”€â”€â”€â”€â”
         â”œâ”€â†’ h0 â”€â†’
  x1â”€â”€â”€â”€â”€â”˜        \
                   â†’ y
  x0â”€â”€â”€â”€â”€â”        /
         â”œâ”€â†’ h1 â”€â†’
  x1â”€â”€â”€â”€â”€â”˜
```

- Hidden neurons use sigmoid activation to carve nonâ€‘linear boundaries.  
- Output neuron uses sigmoid to produce a continuous [0,1] output.

---

## ğŸ”§ Configuration

Within the script you can adjust:

- **`lr`** (learning rate)  
- **`epochs`** (total training iterations)  
- **Snapshot frequency** (currently everyÂ 100 epochs)  
- **Mesh resolution** (grid size for decision surface)

---

## ğŸ¤ Contributing

Feel free to:

- Add support for different activation functions  
- Experiment with more hidden neurons or deeper networks  
- Export to MP4 or interactive HTML  

Pull requests and issues are welcome!

---

## ğŸ“„ License

This project is released under the [MIT License](LICENSE).

---

*Happy learning!*

